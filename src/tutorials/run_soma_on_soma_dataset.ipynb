{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solving SOMA MoCap Dataset\n",
    "SOMA code uses [OmegaConf](https://omegaconf.readthedocs.io/en/2.1_branch/) to control different settings\n",
    "while separating code from configuration files. You can find the configuration file for training SOMA at\n",
    "```` soma/support_data/conf/soma_train_conf.yaml ````\n",
    "\n",
    "You can change every value of the configuration inside the Jupyter, so you do not need to change the YAML file,\n",
    "unless you want to change the default value for future cases.\n",
    "\n",
    "\n",
    "## SOMA MoCap Dataset\n",
    "SOMA dataset consists of two male subjects. You can obtain the unlabeled mocap point cloud (MPC) data from\n",
    "[project's download webpage](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=evaluation_mocaps/original/SOMA_dataset/SOMA_unlabeled_mpc.tar.bz2).\n",
    "Place them under ```` support_files/evaluation_mocaps/original ````.\n",
    "\n",
    "Also, get the\n",
    "[markerlayout for SOMA dataset](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=smplx/marker_layouts/SOMA.tar.bz2)\n",
    "and place it under ```` support_files/marker_layouts ````.\n",
    "\n",
    "## Prepare Body Dataset For Training\n",
    "To create a synthetic mocap dataset, first, we need synthetic SMPL-X bodies in gender-neutral format.\n",
    "SOMA is originally trained with body parameters corresponding to\n",
    "ACCAD, CMU, HumanEVA, PosePrior, Total Capture, and Transitions datasets obtainable\n",
    "from [AMASS downloads page](https://amass.is.tue.mpg.de/download.php).\n",
    "Place the files under the directory identified by\n",
    "```` dirs.amass_dir ````; e.g. ```` support_files/smplx/amass_neutral ````.\n",
    "\n",
    "SOMA will turn these AMASS bodies into PyTorch pt files and h5 ones.\n",
    "\n",
    "Alternatively, you can directly download the\n",
    "[training body parameters without CAESAR subjects](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=smplx/training_body_parameters/body_dataset.tar.bz2)\n",
    "and place it under:\n",
    "```` dirs.body_dataset_dir ````; e.g. ```` support_files/smplx/body_dataset ````.\n",
    "\n",
    "## Prepare Body Model and Co.\n",
    "Obtain a SMPL-X locked head body model for SOMA from [this link](https://smpl-x.is.tue.mpg.de/download.php).\n",
    "Download the [extra smplx data](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=smplx/extra_smplx_data.tar.bz2)\n",
    "and place it in the smplx folder as you see in the above image.\n",
    "\n",
    "Download the\n",
    "[SSM head marker covariances](https://download.is.tue.mpg.de/soma/ssm_head_marker_corr.npz) and place it inside\n",
    "```` dirs.support_base_dir ````; e.g. ```` support_files/ ````.\n",
    "\n",
    "## Notes\n",
    "- Due to licensing restrictions we cannot release the AMASS marker noise model and the CAESAR beta parameters.\n",
    "In an ablative study in the paper, we have shown that these parameters improve the performance SOMA.\n",
    "So without them it might be that the model you train would be underperforming, hence as an alternative, you can obtain the\n",
    "[pretrained SOMA model for the SOMA dataset](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=training_experiments/V48_02_SOMA.tar.bz2). \n",
    "We also release pretrained models for all the SOMA paper experiments.\n",
    "- Addresses like ```` dirs.support_base_dir ```` point to a configuration key in the training YAML file; i.e.\n",
    "```` soma/support_data/conf/soma_train_conf.yaml ````.\n",
    "Check out the YAML file to learn more about the configurable settings.\n",
    "All the key hierarchy is **dot accessible**, and we will show how to overload their values inside the code.\n",
    "- The markerlayout could be only a c3d file in which case SOMA will run MoSh first to obtain the JSON file.\n",
    "The markerlayout file you have already downloaded contains the JSON file.\n",
    "\n",
    "Downloading stuff and placing it in the right location will hopefully pay pff; so just hang on :)\n",
    "\n",
    "From here on, we will assume your uncompressed files and directories looks like this:\n",
    "\n",
    "<img alt=\"alt text\" height=\"256\" src=\"https://download.is.tue.mpg.de/soma/tutorials/tutorial_training_folder_structure.png\"\n",
    "title=\"a mocap superset with 89 markers\" width=\"256\"/>\n",
    "\n",
    "## Training SOMA\n",
    "We have prepared a function that can train multiple SOMA models with various data settings; i.e.  ```` def train_multiple_soma ````\n",
    "For the sake of this tutorial, we will train only one model using one GPU on the local machine.\n",
    "\n",
    "To run a training experiment you need to decide on an experiment ID. The SOMA model lineup for ICCV'21 is V48_02 [*1].\n",
    "\n",
    "Here we dive into the training code headfirst and later provide further explanation.\n",
    "\n",
    "[*1]: Sure we trained more than 48 variants of SOMA.\n",
    "Along the way, PyTorch lightning helped in reducing environmental impact by providing tools to detect issues early on in the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from soma.train.train_soma_multiple import train_multiple_soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "soma_expr_id = 'V48_02_SOMA'\n",
    "\n",
    "soma_data_settings = [(5, 3, 0.0, 1.0), ] # upto 5 occlusions, upto 3 ghost points, 0.0% real data, 100. % synthetic data\n",
    "soma_work_base_dir = '/home/naveen/Documents/Naveen/soma'\n",
    "support_base_dir = osp.join(soma_work_base_dir, 'support_files')\n",
    "soma_marker_layout_fname = osp.join(support_base_dir, 'marker_layouts/SOMA/soma_subject1/clap_001.c3d')\n",
    "\n",
    "num_gpus = 1 # number of gpus for training\n",
    "num_cpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_multiple_soma(\n",
    "    soma_data_settings=soma_data_settings,\n",
    "    soma_train_cfg={\n",
    "        'soma.expr_id': soma_expr_id, # the experiment ID\n",
    "\n",
    "        'dirs.support_base_dir': support_base_dir,\n",
    "        'dirs.work_base_dir': soma_work_base_dir,\n",
    "        'data_parms.mocap_dataset.amass_marker_noise_model.enable': False, # we cannot create amass marker noise model\n",
    "        \n",
    "        'moshpp_cfg_override.moshpp.verbosity': 1,\n",
    "        'moshpp_cfg_override.dirs.support_base_dir':support_base_dir,\n",
    "\n",
    "        'trainer.fast_dev_run': True, # if true then only one iteration of training and validation is done.\n",
    "\n",
    "        'data_parms.mocap_dataset.marker_layout_fnames': [soma_marker_layout_fname],\n",
    "        'train_parms.batch_size': 256,\n",
    "        'trainer.num_gpus': num_gpus,\n",
    "        'train_parms.num_workers': num_cpus,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The above training code ran for an epoch and stoped because we had  ```` 'trainer.fast_dev_run': True ````.\n",
    "Set this flag to False to run a full training.\n",
    "Instead, let's simply download the already\n",
    "[trained SOMA model](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=training_experiments/V48_02_SOMA.tar.bz2).\n",
    "Replace the weights from the snapshots directory in your local directory and that should be it.\n",
    "Note that we assume you have not changed any of the model settings.\n",
    "If you replace the whole folder you might need to download the corresponding\n",
    " [training data](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=smplx/data/V48_01_SOMA.tar.bz2) and place it in the *data* folder.\n",
    "The data folder keeps the settings required for trained model to work at runtime.\n",
    "\n",
    "Note how we control the amount of noise during training; i.e. upto a number of ghost points or occlusions or ratio of real vs synthetic data.\n",
    "For that we set *soma_data_settings* defined in the code above.\n",
    "You can also change the distribution of ghost points by simply setting\n",
    "```` 'data_parms.mocap_dataset.ghost_distribution' ```` a value from spherical_gaussian, uniform, skewed_gaussian.\n",
    "Or you can disable random marker placement by setting ```` 'data_parms.marker_dataset.num_random_vid_ring': 0 ````.\n",
    "Please have a look at the ```` soma_train_conf.yaml ```` to learn more about the possibilities.\n",
    "\n",
    "You have control on the MoSh used to estimate the markerlayout by the keys under ```` moshpp_cfg_override ````.\n",
    "The descendant keys actually override settings of MoSh configuration that has default values saved under\n",
    "```` moshpp/support_data/conf/moshpp_conf.yaml ```` which is in the [moshpp repository](https://github.com/nghorbani/moshpp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running SOMA On MoCap Point Cloud Data\n",
    "The interface to run SOMA post training is the ```` run_soma_on_multiple_settings ````\n",
    "that does exactly what its name suggests! i.e. run multiple SOMA models on various settings.\n",
    "Running, includes all aspects of autolabeling, solving, rendering, and computing evaluation metrics.\n",
    "For an overview of the capabilities of this interface refer to its docstring.\n",
    "Let's first autolabel the\n",
    "[SOMA dataset's MPC data](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=evaluation_mocaps/original/SOMA_dataset/SOMA_unlabeled_mpc.tar.bz2). After downloadig the mocaps place it under:\n",
    "```` support_files/evaluation_mocaps/original ````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from soma.train.soma_trainer import create_soma_data_id\n",
    "from soma.run_soma.paper_plots.mosh_soma_dataset import gen_stagei_mocap_fnames\n",
    "from soma.tools.run_soma_multiple import run_soma_on_multiple_settings\n",
    "soma_data_ids = [create_soma_data_id(*soma_data_setting) for soma_data_setting in soma_data_settings]\n",
    "print(soma_data_ids)\n",
    "mocap_base_dir = osp.join(support_base_dir, 'evaluation_mocaps/original')\n",
    "soma_mocap_target_ds_name = 'SOMA_unlabeled_mpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_soma_on_multiple_settings(\n",
    "        soma_expr_ids=[soma_expr_id],\n",
    "        soma_mocap_target_ds_names=[\n",
    "            'SOMA_unlabeled_mpc',\n",
    "        ],\n",
    "        soma_data_ids=soma_data_ids,\n",
    "        soma_cfg={\n",
    "            'soma.batch_size': 512,\n",
    "            'dirs.support_base_dir': support_base_dir,\n",
    "            'mocap.unit': 'mm',\n",
    "            'save_c3d': True,\n",
    "            'keep_nan_points': True,  # required for labeling evaluation\n",
    "            'remove_zero_trajectories': False  # required for labeling evaluation\n",
    "        },\n",
    "        mocap_base_dir=mocap_base_dir,\n",
    "        run_tasks=['soma'],\n",
    "\n",
    "        mocap_ext='.c3d',\n",
    "        soma_work_base_dir = soma_work_base_dir,\n",
    "        \n",
    "        parallel_cfg = {\n",
    "            # 'max_num_jobs': 1, # comment to run on all mocaps\n",
    "            'randomly_run_jobs': True,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note how we controlled SOMA configuration; i.e. batch_size, etc.\n",
    "A new folder,\n",
    "\n",
    "```` training_experiments/V48_02_SOMA/OC_05_G_03_real_000_synt_100/evaluations/soma_labeled_mocap_tracklet ````\n",
    "\n",
    "is created by the SOMA runtime code that holds the autolabeled mocaps.\n",
    "\n",
    "Moving on, since we have the manually labeled ground-truth mocaps let's see how the labeling performance is.\n",
    "For this download the\n",
    "[manually labeled ground-truth SOMA dataset mocaps](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=evaluation_mocaps/original/SOMA_dataset/SOMA_manual_labeled.tar.bz2)\n",
    "and uncompress it inside ```` support_files/evaluation_mocaps/original ````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_soma_on_multiple_settings(\n",
    "#         soma_expr_ids=[soma_expr_id],\n",
    "#         soma_mocap_target_ds_names=[soma_mocap_target_ds_name,],\n",
    "#         soma_data_ids=soma_data_ids,\n",
    "#         ds_name_gt='SOMA_manual_labeled',\n",
    "#         mocap_base_dir=mocap_base_dir,\n",
    "#         eval_label_cfg={'dirs.support_base_dir':support_base_dir},\n",
    "#         run_tasks=['eval_label'],\n",
    "# #         fast_dev_run=True,\n",
    "#         mocap_ext='.c3d',\n",
    "#         soma_work_base_dir = soma_work_base_dir,\n",
    "#         parallel_cfg = {\n",
    "# #             'max_num_jobs': 1, # comment to run on all mocaps\n",
    "#             'randomly_run_jobs': True,\n",
    "#         },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the log output above, the green lines show the final result per sequence; i.e. perseq.\n",
    "Meanwhile, SOMA runtime code has created another folder holding the labeling evaluation results; i.e.\n",
    "\n",
    "```` training_experiments/V48_02_SOMA/OC_05_G_03_real_000_synt_100/evaluations/soma_eval_tracklet ````\n",
    "\n",
    "Let's aggregate these results into one number per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_soma_on_multiple_settings(\n",
    "#         soma_expr_ids=[soma_expr_id],\n",
    "#         soma_mocap_target_ds_names=[soma_mocap_target_ds_name,],\n",
    "#         soma_data_ids=soma_data_ids,\n",
    "#         ds_name_gt='SOMA_manual_labeled',\n",
    "#         mocap_base_dir=mocap_base_dir,\n",
    "#         eval_label_cfg={'dirs.support_base_dir':support_base_dir},\n",
    "#         run_tasks=['eval_label_aggregate'],\n",
    "# #         fast_dev_run=True,\n",
    "#         mocap_ext='.c3d',\n",
    "#         soma_work_base_dir = soma_work_base_dir,\n",
    "#         parallel_cfg = {\n",
    "# #             'max_num_jobs': 1, # comment to run on all mocaps\n",
    "#             'randomly_run_jobs': True,\n",
    "#         },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the results for the whole SOMA dataset using the pretrained model which should be the same as the one presented in the paper.\n",
    "Another directory is  created at the root directory\n",
    "holding the Excel file including the aggregated as well as the detailed summary of the labeling evaluation:\n",
    "\n",
    "```` evaluations/SOMA_unlabeled_mpc_V49_01_SOMA_OC_05_G_03_real_000_synt_100_tracklet_labeling.xlsx ````\n",
    "\n",
    "\n",
    "The evaluation excel filenames have the following format:\n",
    "````\n",
    "(A:dataset name)___(B:marker layout)__(C:moocap corrupting noise settings)_(D:experiment id)_(E:training noise model settings)_(F:(tracklet) labeling/v2v).xlsx\n",
    "````\n",
    "As an example\n",
    "\n",
    "````\n",
    "SOMA_unlabeled_mpc_V49_01_SOMA_OC_05_G_03_real_000_synt_100_tracklet_labeling.xlsx\n",
    "````\n",
    "\n",
    "could be segmented into:\n",
    "````\n",
    "(A:SOMA_unlabeled_mpc)_(B: not used in real data)_(C:not used in read data)_(D:V49_01_SOMA)_(E:OC_05_G_30_real_000_synt_100)_(F:tracklet labeling).xlsx_\n",
    "````\n",
    "\n",
    "\n",
    "Next we will run MoSh++ on the autolabeled mocaps and get the solved bodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Bodies with MoSh++\n",
    "\n",
    "During the first stage, MoSh++ uses 12 frames randomly selected from subject specific mocaps to estimate the shape of the subject and\n",
    " placement of the markers.\n",
    "These 12 frames should have the same frame number so that the results for the second stage of MoSh++ would be comparable.\n",
    "To achieve this we use *gen_stagei_mocap_fnames* for the SOMA dataset mocaps.\n",
    "Here we see flexibility of the SOMA runtime code in new scenarios.\n",
    "\n",
    "Current MoSh code runs on single **CPU** hence it is very slow. Specially the first stage of MoSh would be the slowest.\n",
    "Therefore, we run the code over the first 10 frames for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subject_name in [\n",
    "    'soma_subject1',\n",
    "    'soma_subject2' # uncomment to process this subject as well\n",
    "]:\n",
    "    mocap_dir = osp.join(soma_work_base_dir,\n",
    "                         'training_experiments',\n",
    "                         soma_expr_id, soma_data_ids[0],\n",
    "                         'evaluations',\n",
    "                         'soma_labeled_mocap_tracklet',\n",
    "                         soma_mocap_target_ds_name)\n",
    "    # stagei_mocap_fnames = gen_stagei_mocap_fnames(mocap_dir, subject_name, ext='.pkl')\n",
    "\n",
    "    run_soma_on_multiple_settings(\n",
    "        soma_expr_ids=[\n",
    "            soma_expr_id,\n",
    "        ],\n",
    "        soma_mocap_target_ds_names=[\n",
    "            'SOMA_unlabeled_mpc',\n",
    "        ],\n",
    "        soma_data_ids=\n",
    "        soma_data_ids,\n",
    "        mosh_cfg={\n",
    "            'moshpp.verbosity': 1,  # set to two to visualize the process in psbody.mesh.mesh_viewer\n",
    "            # 'moshpp.stagei_frame_picker.stagei_mocap_fnames': stagei_mocap_fnames,\n",
    "            # 'moshpp.stagei_frame_picker.type': 'manual',\n",
    "            'moshpp.stagei_frame_picker.type': 'random',\n",
    "\n",
    "            'dirs.support_base_dir': support_base_dir,\n",
    "\n",
    "            # 'mocap.end_fidx': 10  # comment in real runs\n",
    "        },\n",
    "        mocap_base_dir=mocap_base_dir,\n",
    "        run_tasks=['mosh'],\n",
    "        fname_filter=[subject_name],\n",
    "        #         fast_dev_run=True,\n",
    "        mocap_ext='.c3d',\n",
    "        soma_work_base_dir=soma_work_base_dir,\n",
    "        parallel_cfg={\n",
    "            # 'max_num_jobs': 1,  # comment to run on all mocaps\n",
    "            'randomly_run_jobs': True,\n",
    "        },\n",
    "\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the already\n",
    "[computed MoSh++ results for the whole SOMA dataset using the SOMA autolabeled mocaps](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=evaluation_mocaps/original/SOMA_dataset/mosh_results/SOMA_soma_autolabeled.tar.bz2)\n",
    "and place them inside the experiment folder, substituting the contents already created at\n",
    "```` training_experiments/V48_02_SOMA/OC_05_G_03_real_000_synt_100/evaluations/mosh_results_tracklet ````.\n",
    "\n",
    "Now download the \n",
    "[MoSh++ results for manually labeled mocaps](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=evaluation_mocaps/original/SOMA_dataset/mosh_results/SOMA_manual_labeled.tar.bz2)\n",
    "and place them at the root directory; i.e.\n",
    "```` support_files/mosh_results/SOMA_manual_labeled  ````.\n",
    "Note that here we created a *moh_results* directory for better organization of the files.\n",
    "\n",
    "Now we can actually compute vertex-to-vertex error in reconstructing the body surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gt_mosh_base_dir= osp.join(support_base_dir, 'mosh_results')\n",
    "\n",
    "# run_soma_on_multiple_settings(\n",
    "#         soma_expr_ids=[soma_expr_id],\n",
    "#         soma_mocap_target_ds_names=[soma_mocap_target_ds_name,],\n",
    "#         soma_data_ids=soma_data_ids,\n",
    "#         ds_name_gt='SOMA_manual_labeled',\n",
    "#         mocap_base_dir=mocap_base_dir,\n",
    "#         eval_v2v_cfg={'dirs.support_base_dir':support_base_dir},\n",
    "\n",
    "#         run_tasks=['eval_v2v'],\n",
    "\n",
    "#         mocap_ext='.c3d',\n",
    "#         gt_mosh_base_dir=gt_mosh_base_dir,\n",
    "#         soma_work_base_dir = soma_work_base_dir,\n",
    "#         parallel_cfg = {\n",
    "#             # 'max_num_jobs': 3, # comment to run on all mocaps\n",
    "#             'randomly_run_jobs': True,\n",
    "#         },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the labeling evaluations we have to run the SOMA interface once more with\n",
    "*eval_v2v_aggregate* to aggregate the perseq results into one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run_soma_on_multiple_settings(\n",
    "#         soma_expr_ids=[soma_expr_id],\n",
    "#         soma_mocap_target_ds_names=[soma_mocap_target_ds_name,],\n",
    "#         soma_data_ids=soma_data_ids,\n",
    "#         ds_name_gt='SOMA_manual_labeled',\n",
    "#         mocap_base_dir=mocap_base_dir,\n",
    "#         run_tasks=['eval_v2v_aggregate'],\n",
    "\n",
    "#         # fast_dev_run=True,\n",
    "\n",
    "#         mocap_ext='.c3d',\n",
    "#         gt_mosh_base_dir=gt_mosh_base_dir,\n",
    "\n",
    "#         soma_work_base_dir = soma_work_base_dir,\n",
    "#         parallel_cfg = {\n",
    "# #             'max_num_jobs': 1, # comment to run on all mocaps\n",
    "#             'randomly_run_jobs': True,\n",
    "#         },\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers follow the ones presented in the paper.\n",
    "\n",
    "So far we have autolabeled, and solved the given mocaps.\n",
    "Furthermore, we have evaluated our SOMA results with regard to labeling performance and surface reconstruction accuracy.\n",
    "New let's see how we can render the solved bodies.\n",
    "\n",
    "## Rendering Solved Bodies\n",
    "\n",
    "If you have installed Blender 2.83-LTS following the installation instructions you can also render solved bodies using Blender.  Download the \n",
    "[Blender blend files](https://download.is.tue.mpg.de/download.php?domain=soma&sfile=blender/blend_files.tar.bz2)\n",
    "and place them under\n",
    "```` support_files/blender/blend_files ````.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_temp_dir = osp.join(soma_work_base_dir, 'blender_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_soma_on_multiple_settings(\n",
    "        soma_expr_ids=[soma_expr_id],\n",
    "        soma_mocap_target_ds_names=[\n",
    "            'SOMA_unlabeled_mpc',\n",
    "        ],\n",
    "        soma_data_ids=soma_data_ids,\n",
    "        render_cfg={\n",
    "            'moshpp.verbosity': 1,\n",
    "            # 'render.render_only_one_image': True, # uncomment for initial testing of the pipeline\n",
    "            'render.show_markers': True,\n",
    "            'render.video_fps': 15,  # 25,\n",
    "            'mesh.ds_rate': 5,\n",
    "            'render.save_final_blend_file': False,\n",
    "            'render.resolution.change_from_blend': True,\n",
    "            'render.resolution.default': [1600, 1600],  # [x,y]\n",
    "            'render.render_engine': 'eevee',  # eevee / cycles,\n",
    "            'dirs.temp_base_dir': blender_temp_dir,\n",
    "            'dirs.support_base_dir': support_base_dir,\n",
    "\n",
    "        },\n",
    "        mocap_base_dir=mocap_base_dir,\n",
    "        run_tasks=['render'],\n",
    "\n",
    "        parallel_cfg = {\n",
    "            # 'max_num_jobs': 1, # comment to run on all mocaps\n",
    "            'randomly_run_jobs': True,\n",
    "        },\n",
    "\n",
    "        mocap_ext='.c3d',\n",
    "        soma_work_base_dir = soma_work_base_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "render_image_fnames = glob(osp.join(blender_temp_dir, 'SOMA_unlabeled_mpc', 'png_files', 'soma_standard', '*/*.png'))\n",
    "print(f'Found {len(render_image_fnames)} rendered images. showing one!')\n",
    "\n",
    "random.shuffle(render_image_fnames)\n",
    "Image(filename=render_image_fnames[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the\n",
    "[AMASS tutorials](https://github.com/nghorbani/amass/blob/master/notebooks/01-AMASS_Visualization.ipynb)\n",
    "and the [body visualizer](https://github.com/nghorbani/body_visualizer)\n",
    " to turn mosh pkl files into AMASS npz format on the fly and render in Jupyter. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from moshpp.mosh_head import MoSh\n",
    "import numpy as np\n",
    "\n",
    "mosh_stageii_pkl_fname = osp.join(soma_work_base_dir, 'training_experiments/V48_02_SOMA/OC_05_G_03_real_000_synt_100/evaluations/mosh_results_tracklet/SOMA_unlabeled_mpc/soma_subject1/clap_001_stageii.pkl')\n",
    "mosh_result = MoSh.load_as_amass_npz(mosh_stageii_pkl_fname, include_markers=True)\n",
    "print({k:v if isinstance(v, str) or isinstance(v,float) or isinstance(v,int) else v.shape for k,v in mosh_result.items() if not isinstance(v, list) and not isinstance(v,dict)})\n",
    "\n",
    "time_length = len(mosh_result['trans'])\n",
    "mosh_result['betas'] = np.repeat(mosh_result['betas'][None], repeats=time_length, axis=0)\n",
    "\n",
    "subject_gender = mosh_result['gender']\n",
    "surface_model_type = mosh_result['surface_model_type']\n",
    "print(f'subject_gender: {subject_gender}, surface_model_type: {surface_model_type}, time_length: {time_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from body_visualizer.tools.vis_tools import colors\n",
    "from body_visualizer.mesh.mesh_viewer import MeshViewer\n",
    "from body_visualizer.mesh.sphere import points_to_spheres\n",
    "from body_visualizer.tools.vis_tools import show_image\n",
    "\n",
    "imw, imh=1600, 1600\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "import torch\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c \n",
    "\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "bm_fname = osp.join(soma_work_base_dir, f'support_files/{surface_model_type}/{subject_gender}/model.npz')\n",
    "\n",
    "num_betas = mosh_result['num_betas'] # number of body parameters\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname, num_betas=num_betas).to(comp_device)\n",
    "faces = c2c(bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parms = {k:torch.Tensor(v).to(comp_device) for k,v in mosh_result.items() if k in ['pose_body', 'betas', 'pose_hand']}\n",
    "print({k:v.shape for k,v in body_parms.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "body_pose_hand = bm(**body_parms)\n",
    "\n",
    "def vis_body_pose_hand(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_pose_hand.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "vis_body_pose_hand(fId=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the *fId* to render another frame. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
